


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import KNNImputer
from collections import defaultdict # used to store the label encoder of multiple features and inverse_transform
import seaborn as sns


import os
for dirname, _, filenames in os.walk('./input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))





train = pd.read_csv(r'./input/train.csv')\
    .drop(columns='Id') # 'Id' was just a repetion of the index
test = pd.read_csv(r'./input/test.csv')\
    .drop(columns='Id') # 'Id' was just a repetion of the index


train.info()


train.dtypes.unique()


print('# of obj columns:', train.select_dtypes(include='O').columns.size) 
print('# of num columns:', train.select_dtypes(exclude='O').columns.size) 





sns.histplot(train['SalePrice'])








#ordinal enconding, hard-coded for e.g. quality categories (Excellent, Good, Fair, Poor, etc...)
ord_ft = {
    'LotShape': {'Reg': 1, 'IR1': 2, 'IR2': 3,'IR3': 4},
    'LandSlope': {'Gtl': 1, 'Mod': 2, 'Sev': 3},
    'ExterQual': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5},
    'ExterCond': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5},
    'BsmtQual': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5,'NA': 6}, 
    'BsmtCond': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5,'NA': 6},
    'BsmtExposure': {'Gd': 1, 'Av': 2, 'Mn': 3, 'No': 4,'NA': 5},
    'BsmtFinType1': {'GLQ': 1, 'ALQ': 2, 'BLQ': 3, 'Rec': 4,'LwQ': 5,'Unf': 6,'NA': 7},
    'BsmtFinType2': {'GLQ': 1, 'ALQ': 2, 'BLQ': 3, 'Rec': 4,'LwQ': 5,'Unf': 6,'NA': 7},
    'HeatingQC': {'Ex': 1, 'Gd': 2, 'TA': 3,'Fa': 4, 'Po': 5},
    'Electrical': {'SBrkr': 1, 'FuseA': 2, 'FuseF': 3, 'FuseP': 4,'Mix': 5},
    'KitchenQual': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5},
    'Functional': {'Typ': 1, 'Min1': 2, 'Min2': 3, 'Mod': 4,'Maj1': 5,'Maj2': 6,'Sev': 7,'Sal': 8},
    'FireplaceQu': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5,'NA': 6},
    'GarageFinish': {'Fin': 1, 'RFn': 2, 'Unf': 3, 'NA': 4},
    'GarageQual': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5,'NA': 6},
    'GarageCond': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'Po': 5,'NA': 6},
    'PavedDrive': {'Y': 1, 'P': 2, 'N': 3},
    'PoolQC': {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4,'NA': 5},
    'Fence': {'GdPrv': 1, 'MnPrv': 2, 'GdWo': 3, 'MnWw': 4, 'NA': 5},
    'SaleCondition': {'Normal': 1, 'Abnorml': 2, 'AdjLand': 3, 'Alloca': 4, 'Family': 5, 'Partial': 6},
}

train[list(ord_ft)] = train[list(ord_ft)].fillna('NA')
test[list(ord_ft)] = test[list(ord_ft)].fillna('NA')

train.replace(ord_ft, inplace=True)
test.replace(ord_ft, inplace=True)

train.replace('NA', np.nan, inplace=True)
test.replace('NA', np.nan, inplace=True)


# new column for Basement Yes/No?
train['Bsmt'] = train['BsmtCond'].isna().astype(int)
test['Bsmt'] = test['BsmtCond'].isna().astype(int)

# new column for Garage Yes/No?
train['Garage'] = train['GarageFinish'].isna().astype(int)
test['Garage'] = test['GarageFinish'].isna().astype(int)



# unfolds the 'MiscFeature' into separate columns and drops it
for misc_feat in train['MiscFeature'].dropna().unique():
    train[misc_feat] = (train['MiscFeature'] == misc_feat).astype(int)
    test[misc_feat] = (test['MiscFeature'] == misc_feat).astype(int)

train.drop(columns='MiscFeature', inplace=True)
test.drop(columns='MiscFeature', inplace=True)    



print('# of obj columns:', train.select_dtypes(include='O').columns.size) 
print('# of num columns:', train.select_dtypes(exclude='O').columns.size) 





train_cat = train.select_dtypes(include='O')
test_cat = test.select_dtypes(include='O')
train_cat


d = defaultdict(LabelEncoder) # structure to add the label encoders for each categorical feature (df column)

# Encoding the variable
train_cat_enc = train_cat.apply(lambda x: d[x.name].fit_transform(x))
test_cat_enc = test_cat.apply(lambda x: d[x.name].fit_transform(x))
train_cat_enc

# Inverse the encoded
#train_cat_enc.apply(lambda x: d[x.name].inverse_transform(x))
#test_cat_enc.apply(lambda x: d[x.name].inverse_transform(x))

# Using the dictionary to label future data
#new_data.apply(lambda x: d[x.name].transform(x))


train_clean = pd.concat([train.select_dtypes(exclude='O'), train_cat_enc], axis=1)
test_clean = pd.concat([test.select_dtypes(exclude='O'), test_cat_enc], axis=1)


train_clean.select_dtypes(include=float)





train_clean.isna().sum().sort_values(ascending=False)[:10]/train.shape[0]


test_clean.isna().sum().sort_values(ascending=False)[:10]/train.shape[0]


impute_cols_train = train_clean.columns[train_clean.isna().any()]
impute_cols_test = test_clean.columns[test_clean.isna().any()]

print(impute_cols_test)
print(impute_cols_train)


imputer = KNNImputer(n_neighbors=20, weights='distance')
train_imp = pd.DataFrame(data=imputer.fit_transform(train_clean), index=train_clean.index, columns=train_clean.columns)
train_clean = train_imp
test_imp = pd.DataFrame(data=imputer.fit_transform(test_clean), index=test_clean.index, columns=test_clean.columns)
test_clean = train_imp


train_clean.isna().any(axis=None)


test_clean.isna().any(axis=None)





# standard scaling 


scaler = StandardScaler()
scaler.fit(train_clean);


X_train = pd.DataFrame(scaler.transform(train_clean), index=train_clean.index, columns=train_clean.columns)
X_test = pd.DataFrame(scaler.transform(test_clean), index=test_clean.index, columns=test_clean.columns)


X_train


from sklearn.preprocessing import PolynomialFeatures












